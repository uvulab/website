(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{149:function(e,t,a){e.exports=a.p+"static/media/amir.cbc0b213.jpg"},150:function(e,t,a){e.exports=a.p+"static/media/amir_and_kids.d276106f.jpg"},163:function(e,t,a){e.exports=a(309)},170:function(e,t,a){},308:function(e,t,a){"use strict";a.r(t);var n=a(0),i=a.n(n),r=a(328),o=a(317),s=a(153),c=a(90),l=a(91),h=a(96),m=a(152),u=a(92);function d(e){var t=e.device,a={};return"desktop"===(void 0===t?"desktop":t)?a.height=40.7:a.width=118,i.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",x:"0",y:"0",version:"1.1",viewBox:"0 0 319.78 133.38",xmlSpace:"preserve",style:a},i.a.createElement("linearGradient",{id:"SVGID_1_",x1:"49.745",x2:"49.745",y1:"109.333",y2:"0",gradientUnits:"userSpaceOnUse"},i.a.createElement("stop",{offset:"0",stopColor:"#00A8DE"}),i.a.createElement("stop",{offset:"0.4",stopColor:"#333391"}),i.a.createElement("stop",{offset:"1",stopColor:"#E91388"})),i.a.createElement("path",{fill:"none",stroke:"url(#SVGID_1_)",strokeMiterlimit:"10",strokeWidth:"20",d:"M10.409999999999997 0v60m0 0c0 21.72 17.61 39.33 39.33 39.33S89.08 81.72 89.08 60m0-60v60"}),i.a.createElement("linearGradient",{id:"SVGID_2_",x1:"159.903",x2:"159.903",y1:"133.382",y2:"49.743",gradientUnits:"userSpaceOnUse"},i.a.createElement("stop",{offset:"0",stopColor:"#00A8DE"}),i.a.createElement("stop",{offset:"0.4",stopColor:"#333391"}),i.a.createElement("stop",{offset:"1",stopColor:"#E91388"})),i.a.createElement("path",{fill:"url(#SVGID_2_)",d:"M160.21 133.38L113.95 49.84 136.76 49.74 160.09 91.88 183.07 49.79 205.85 49.79z"}),i.a.createElement("linearGradient",{id:"SVGID_3_",x1:"270.033",x2:"270.033",y1:"109.333",y2:"0",gradientUnits:"userSpaceOnUse"},i.a.createElement("stop",{offset:"0",stopColor:"#00A8DE"}),i.a.createElement("stop",{offset:"0.4",stopColor:"#333391"}),i.a.createElement("stop",{offset:"1",stopColor:"#E91388"})),i.a.createElement("path",{fill:"none",stroke:"url(#SVGID_3_)",strokeMiterlimit:"10",strokeWidth:"20",d:"M230.70000000000002 0v60m0 0c0 21.72 17.61 39.33 39.33 39.33S309.37 81.72 309.37 60m0-60v60"}))}var p=a(329),g=a(314),f=a(326),b=a(322),v=a(315),E=a(321),y=a(97),w=a(316),k=a(136);function x(){return i.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"200",height:"200",viewBox:"0 0 800 800"},i.a.createElement("path",{fill:"#260133",d:"M0 0H800V800H0z"}),i.a.createElement("g",{fill:"none",stroke:"#404"},i.a.createElement("path",{d:"M769 229l268 31.9M927 880L731 737l-211-77-211-122-269 61 255 165-168.5 115.5L40 599l-237-106 299-111-133-153L126.5 79.5-69-63"}),i.a.createElement("path",{d:"M-31 229l268 32 153 121 213 111-294.5 44.5-207-156M370 905l-75-141"}),i.a.createElement("path",{d:"M520 660l58 182 153-105 109-138-237-106-83 167-225 104 14-226 81-156 149-113 230-40L577.5 41.5 370 105 295-36 126.5 79.5 237 261 102 382 40 599-69 737l196 143"}),i.a.createElement("path",{d:"M520-140l58.5 182.5L731-63M603 493l-64-224-302-8 133-156m532 277L539 269M390 382H102"}),i.a.createElement("path",{d:"M-222 42l348.5 37.5L370 105l169 164 38.5-227.5L927 80 769 229l133 153-299 111 128 244M295-36l282.5 77.5M578 842l-283-78M40-201l87 281m-25 302l-363-113"})),i.a.createElement("g",{fill:"#505"},i.a.createElement("circle",{cx:"769",cy:"229",r:"5"}),i.a.createElement("circle",{cx:"539",cy:"269",r:"5"}),i.a.createElement("circle",{cx:"603",cy:"493",r:"5"}),i.a.createElement("circle",{cx:"731",cy:"737",r:"5"}),i.a.createElement("circle",{cx:"520",cy:"660",r:"5"}),i.a.createElement("circle",{cx:"309",cy:"538",r:"5"}),i.a.createElement("circle",{cx:"295",cy:"764",r:"5"}),i.a.createElement("circle",{cx:"40",cy:"599",r:"5"}),i.a.createElement("circle",{cx:"102",cy:"382",r:"5"}),i.a.createElement("circle",{cx:"127",cy:"80",r:"5"}),i.a.createElement("circle",{cx:"370",cy:"105",r:"5"}),i.a.createElement("circle",{cx:"578",cy:"42",r:"5"}),i.a.createElement("circle",{cx:"237",cy:"261",r:"5"}),i.a.createElement("circle",{cx:"390",cy:"382",r:"5"})))}function S(e){var t=function(){if("undefined"===typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"===typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],function(){})),!0}catch(e){return!1}}();return function(){var a,n=Object(u.a)(e);if(t){var i=Object(u.a)(this).constructor;a=Reflect.construct(n,arguments,i)}else a=n.apply(this,arguments);return Object(m.a)(this,a)}}var C=encodeURIComponent(Object(k.renderToStaticMarkup)(i.a.createElement(x,null))),I=function(){return"undefined"===typeof window?p.a.onlyTablet.minWidth:window.innerWidth},P=function(e){Object(h.a)(a,e);var t=S(a);function a(){var e;Object(c.a)(this,a);for(var n=arguments.length,i=new Array(n),r=0;r<n;r++)i[r]=arguments[r];return(e=t.call.apply(t,[this].concat(i))).state={},e.hideFixedMenu=function(){return e.setState({fixed:!1})},e.showFixedMenu=function(){return e.setState({fixed:!0})},e}return Object(l.a)(a,[{key:"render",value:function(){var e=this.props,t=e.children,a=e.location.pathname,n=this.state.fixed;return i.a.createElement(p.a,{getWidth:I,minWidth:p.a.onlyTablet.minWidth,style:{width:"100%",height:"100%",backgroundImage:"url('data:image/svg+xml;utf8, ".concat(C,"')")}},i.a.createElement(g.a,{once:!1,onBottomPassed:this.showFixedMenu,onBottomPassedReverse:this.hideFixedMenu},i.a.createElement(f.a,{inverted:!0,textAlign:"center",vertical:!0},i.a.createElement(b.a,{fixed:n?"top":null,inverted:!n,pointing:!n,secondary:!n,size:"large"},i.a.createElement(v.a,null,i.a.createElement(b.a.Item,{as:w.a,to:"/",style:{paddingTop:0,paddingBottom:0}},i.a.createElement(d,{device:"desktop"})),i.a.createElement(b.a.Item,{as:w.a,to:"/about",active:"/about"===a},"About"),i.a.createElement(b.a.Item,{as:w.a,to:"/people",active:"/people"===a},"People"),i.a.createElement(b.a.Item,{as:w.a,to:"/projects",active:"/projects"===a},"Projects"),i.a.createElement(b.a.Item,{as:w.a,to:"/publications",active:"/publications"===a},"Publications"))))),t)}}]),a}(n.Component),T=function(e){Object(h.a)(a,e);var t=S(a);function a(){var e;Object(c.a)(this,a);for(var n=arguments.length,i=new Array(n),r=0;r<n;r++)i[r]=arguments[r];return(e=t.call.apply(t,[this].concat(i))).state={},e.handleSidebarHide=function(){return e.setState({sidebarOpened:!1})},e.handleToggle=function(){return e.setState({sidebarOpened:!0})},e}return Object(l.a)(a,[{key:"render",value:function(){var e=this.props,t=e.children,a=e.location.pathname,n=this.state.sidebarOpened;return i.a.createElement(p.a,{as:E.a.Pushable,getWidth:I,maxWidth:p.a.onlyMobile.maxWidth,style:{width:"100%",height:"100%",backgroundImage:"url('data:image/svg+xml;utf8, ".concat(C,"')")}},i.a.createElement(E.a,{as:b.a,animation:"push",inverted:!0,onHide:this.handleSidebarHide,vertical:!0,visible:n,width:"thin"},i.a.createElement(b.a.Item,{as:w.a,to:"/",onClick:this.handleSidebarHide},i.a.createElement(d,{device:"mobile"})),i.a.createElement(b.a.Item,{as:w.a,to:"/about",active:"/about"===a,onClick:this.handleSidebarHide},"About"),i.a.createElement(b.a.Item,{as:w.a,to:"/people",active:"/people"===a,onClick:this.handleSidebarHide},"People"),i.a.createElement(b.a.Item,{as:w.a,to:"/projects",active:"/projects"===a,onClick:this.handleSidebarHide},"Projects"),i.a.createElement(b.a.Item,{as:w.a,to:"/publications",active:"/publications"===a,onClick:this.handleSidebarHide},"Publications")),i.a.createElement(E.a.Pusher,{dimmed:n},i.a.createElement(f.a,{inverted:!0,textAlign:"center",vertical:!0},i.a.createElement(v.a,null,i.a.createElement(b.a,{inverted:!0,pointing:!0,secondary:!0,size:"large"},i.a.createElement(b.a.Item,{onClick:this.handleToggle},i.a.createElement(y.a,{name:"sidebar"}))))),t))}}]),a}(n.Component),A=function(e){var t=e.children,a=Object(s.a)(e,["children"]);return i.a.createElement("div",{style:{height:"100%"}},i.a.createElement(P,a,t),i.a.createElement(T,a,t))},j=(0,a(327).a)(A),M=a(318),D=a(324),F=a(325),W=a(28),G=a(149),R=a.n(G);var H={path:"/",component:function(){return i.a.createElement(v.a,{style:{background:"#fff",boxShadow:"0 1px 2px #ccc",width:"80%"}},i.a.createElement(W.a,{style:{height:"calc(100vh - 72.7031px)"}},i.a.createElement(v.a,{text:!0,style:{padding:"1em 0em"}},i.a.createElement(M.a,{src:R.a,bordered:!0,centered:!0,size:"medium"}),i.a.createElement(D.a,{as:"h1",textAlign:"center",style:{marginBottom:"0"}},"Amir Sadovnik"),i.a.createElement(D.a,{as:"h5",textAlign:"center",style:{marginTop:"0"}},"Assistant Professor"),i.a.createElement(F.a,null,i.a.createElement(F.a.Column,{width:8,textAlign:"center"},i.a.createElement("p",null,"Min H. Kao Department of Electrical Engineering & Computer Science ",i.a.createElement("br",null)," The University of Tennessee ",i.a.createElement("br",null),"Knoxville, TN 37996")),i.a.createElement(F.a.Column,{width:8,textAlign:"center"},i.a.createElement("p",null,i.a.createElement("a",{href:"mailto:asadovnik@utk.edu"},"asadovnik@utk.edu")," ",i.a.createElement("br",null)," ",i.a.createElement("a",{href:"tel:865-974-3076"},"865-974-3076")," ",i.a.createElement("br",null)," 352 Min H. Kao Bldg."))))))}},V=a(150),U=a.n(V);var O={path:"/about",component:function(){return i.a.createElement(v.a,{style:{background:"#fff",boxShadow:"0 1px 2px #ccc",width:"80%"}},i.a.createElement(W.a,{style:{height:"calc(100vh - 72.7031px)"}},i.a.createElement(v.a,{text:!0,style:{padding:"1em 0em"}},i.a.createElement(D.a,{as:"h1"},"About Amir"),i.a.createElement(M.a,{src:U.a,bordered:!0}),i.a.createElement(D.a,{as:"h3"},"Education and Teaching"),i.a.createElement("p",null,"I am an assistant professor in the department of Electrical Engineering and Computer Science at the University of Tennessee, Knoxville. Prior to that I was an assistant professor at Lafayette College. I received my PhD from the"," ",i.a.createElement("a",{href:"http://www.ece.cornell.edu/"},"School of Electrical and Computer Engineering at Cornell University"),". I was advised by"," ",i.a.createElement("a",{href:"http://www.ece.cornell.edu/peo-detail.cfm?NetID=tc345"},"Prof. Tsuhan Chen")," ","as member of the Advanced Multimedia Processing Lab. Prior to arriving at Cornell I received my Bachelors in Electrical and Computer Engineering from"," ",i.a.createElement("a",{href:"http://cooper.edu/"},"The Cooper Union"),"."),i.a.createElement(D.a,{as:"h3"},"Research"),i.a.createElement("p",null,"My research in the field of Computer Vision has been mostly driven by the way humans understand and interact with images. This human-centered view has led me to work on new and exciting projects, which utilize tools from different fields (such as Computer Vision, Signal Processing, Natural Language Processing, Machine Learning, etc.) and apply them in new ways."),i.a.createElement(D.a,{as:"h3"},"Hobbies"),i.a.createElement("p",null,"In my spare time I enjoy hanging out with my family, playing guitar, hiking, and going to the movies."))))}},_=a(320);var B=function(e){var t=e.person,a=t.name,n=t.meta,r=t.description,o=t.extra;return i.a.createElement(_.a,{style:{textAlign:"center",margin:"1em auto"}},i.a.createElement(_.a.Content,null,i.a.createElement(_.a.Header,null,a),i.a.createElement(_.a.Meta,null,i.a.createElement("span",null,n)),r&&i.a.createElement(_.a.Description,null,r)),o&&i.a.createElement(_.a.Content,{extra:!0},o))},K=[{type:"Professor",name:"Amir Sadovnik",meta:"Assistant Professor",description:i.a.createElement(n.Fragment,null,"Min H. Kao Department of Electrical Engineering & Computer Science",i.a.createElement("br",null),"The University of Tennessee Knoxville, TN 37996"),extra:i.a.createElement(n.Fragment,null,i.a.createElement("a",{href:"mailto:asadovnik@utk.edu"},"asadovnik@utk.edu")," ",i.a.createElement("a",{href:"tel:865-974-3076"},"865-974-3076"),i.a.createElement("br",null),"352 Min H. Kao Bldg")},{type:"PhD",name:"Jerry Duncan",meta:i.a.createElement(n.Fragment,null,"2",i.a.createElement("sup",null,"nd")," Year PhD Student"),description:i.a.createElement(n.Fragment,null,"Min H. Kao Department of Electrical Engineering & Computer Science",i.a.createElement("br",null),"The University of Tennessee Knoxville, TN 37996"),extra:i.a.createElement(n.Fragment,null,i.a.createElement("a",{href:"mailto:jdunca51@utk.edu"},"jdunca51@utk.edu")," ")},{type:"PhD",name:"Fabian Fallas",meta:i.a.createElement(n.Fragment,null,"2",i.a.createElement("sup",null,"nd")," Year PhD Student"),description:i.a.createElement(n.Fragment,null,"Min H. Kao Department of Electrical Engineering & Computer Science",i.a.createElement("br",null),"The University of Tennessee Knoxville, TN 37996"),extra:i.a.createElement(n.Fragment,null,i.a.createElement("a",{href:"mailto:ffallasm@vols.utk.edu"},"ffallasm@vols.utk.edu")," ")},{type:"PhD",name:"James Senter",meta:i.a.createElement(n.Fragment,null,"3",i.a.createElement("sup",null,"rd")," Year PhD Student"),description:i.a.createElement(n.Fragment,null,"Min H. Kao Department of Electrical Engineering & Computer Science",i.a.createElement("br",null),"The University of Tennessee Knoxville, TN 37996"),extra:i.a.createElement(n.Fragment,null,i.a.createElement("a",{href:"mailto:jsenter3@vols.utk.edu"},"jsenter3@vols.utk.edu")," ")},{type:"PhD",name:"Taher Naderi",meta:i.a.createElement(n.Fragment,null,"4",i.a.createElement("sup",null,"th")," Year PhD Student"),description:i.a.createElement(n.Fragment,null,"Min H. Kao Department of Electrical Engineering & Computer Science",i.a.createElement("br",null),"The University of Tennessee Knoxville, TN 37996"),extra:i.a.createElement(n.Fragment,null,i.a.createElement("a",{href:"mailto:tnaderi@vols.utk.edu"},"tnaderi@vols.utk.edu")," ")},{type:"Undergraduate",name:"Henry Eigen",meta:i.a.createElement(n.Fragment,null,"Junior"),description:i.a.createElement(n.Fragment,null,"Min H. Kao Department of Electrical Engineering & Computer Science",i.a.createElement("br",null),"The University of Tennessee Knoxville, TN 37996"),extra:i.a.createElement(n.Fragment,null,i.a.createElement("a",{href:"mailto:heigen@vols.utk.edu"},"heigen@vols.utk.edu")," ")},{type:"Undergraduate",name:"Sai Thatigotla",meta:i.a.createElement(n.Fragment,null,"Junior"),description:i.a.createElement(n.Fragment,null,"Min H. Kao Department of Electrical Engineering & Computer Science",i.a.createElement("br",null),"The University of Tennessee Knoxville, TN 37996"),extra:i.a.createElement(n.Fragment,null,i.a.createElement("a",{href:"mailto:sthatigo@vols.utk.edu"},"sthatigo@vols.utk.edu")," ")},{type:"Graduated",name:"Natalie Bogda",meta:i.a.createElement(n.Fragment,null,"Master's Degree"),description:void 0,extra:void 0}],L=[{data:K.filter(function(e){return"Professor"===e.type}),label:"Professor",cols:1},{data:K.filter(function(e){return"PhD"===e.type}),label:"PhD Students",cols:4},{data:K.filter(function(e){return"Masters"===e.type}),label:"Master's Students",cols:0},{data:K.filter(function(e){return"Undergraduate"===e.type}),label:"Undergraduates",cols:2},{data:K.filter(function(e){return"Graduated"===e.type}),label:"Graduated",cols:1}].filter(function(e){return e.data.length});function N(e){var t=e.data,a=e.label,n=e.cols;return i.a.createElement(f.a,{vertical:!0},i.a.createElement(F.a,{stackable:!0,container:!0,centered:!0},i.a.createElement(F.a.Row,null,i.a.createElement(D.a,{as:"h2",icon:!0},i.a.createElement(y.a,{name:"student"}),a)),i.a.createElement(F.a.Row,{centered:!0,verticalAlign:"middle",columns:t.length<n?t.length:n},t.map(function(e,t){return i.a.createElement(F.a.Column,{key:t},i.a.createElement(B,{person:e}))}))))}var z={path:"/people",component:function(){return i.a.createElement(v.a,{style:{background:"#fff",boxShadow:"0 1px 2px #ccc",width:"80%"}},i.a.createElement(W.a,{style:{height:"calc(100vh - 72.7031px)"}},L.map(function(e,t){return i.a.createElement(N,Object.assign({},e,{key:t}))})))}},q=a(319);var Y=function(e){var t=e.paper,a=t.author,n=t.coauthors,r=t.pi;return n.length&&(a+=", ".concat(n)),a+=", and ".concat(r),i.a.createElement(q.a,null,i.a.createElement(q.a.Content,null,i.a.createElement(q.a.Header,{as:"a",href:t.link},t.title),i.a.createElement(q.a.Meta,null,a),i.a.createElement(q.a.Description,null,t.abstract),i.a.createElement(q.a.Extra,null,t.citation)))},J=[{author:"Amir Sadovnik",coauthors:"",pi:"Tsuhan Chen",file:"",year:"2011",link:"https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICIP-2011/papers/1569408215.pdf",title:"Pictorial Structures for Object Recognition and Part Labeling in Drawings",citation:"A. Sadovnik, T. Chen. \u201cPictorial Structures for Object Recognition and Part Labeling in Drawings\u201d IEEE International Conference on Image Processing (ICIP) 2011.",abstract:["Although the sketch recognition and computer vision communities attempt to solve similar problems in different domains, the sketch recognition community has not utilized many of the advancements made in computer vision algorithms.","In this paper we propose using a pictorial structure model for object detection, and modify it to better perform in a drawing setting as opposed to photographs.","By using this model we are able to detect a learned object in a general drawing, and correctly label its parts.","We show our results on 4 categories."].join(" ")},{author:"Amir Sadovnik",coauthors:"Yi-I Chiu, Noah Snavely, Shimon Edelman",pi:"Tsuhan Chen",file:"",year:"2012",link:"https://ieeexplore.ieee.org/abstract/document/6248003",title:"Image Description with a Goal: Building Efficient Discriminating Expressions for Images",citation:"A. Sadovnik, Y. Chiu, N. Snavely, S. Edelman and T. Chen. \u201cImage Description with a Goal: Building Efficient Discriminating Expressions for Images\u201d, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.",abstract:["Many works in computer vision attempt to solve different tasks such as object detection, scene recognition or attribute detection, either separately or as a joint problem.","In recent years, there has been a growing interest in combining the results from these different tasks in order to provide a textual description of the scene.","However, when describing a scene, there are many items that can be mentioned.","If we include all the objects, relationships, and attributes that exist in the image, the description would be extremely long and not convey a true understanding of the image.","We present a novel approach to ranking the importance of the items to be described.","Specifically, we focus on the task of discriminating one image from a group of others.","We investigate the factors that contribute to the most efficient description that achieves this task.","We also provide a quantitative method to measure the description quality for this specific task using data from human subjects and show that our method achieves better results than baseline methods."].join(" ")},{author:"Amir Sadovnik",coauthors:"",pi:"Tsuhan Chen",file:"",year:"2012",link:"http://chenlab.ece.cornell.edu/people/Amir/publications/Object_groups.pdf",title:"Hierarchical Object Groups for Scene Classification",citation:"A. Sadovnik and T. Chen, \u201cHierarchical Object Groups for Scene Classification\u201d, IEEE International Conference on Image Processing (ICIP) 2012.",abstract:["The hierarchical structures that exist in natural scenes have been utilized for many tasks in computer vision.","The basic idea is that instead of using strictly low level features it is possible to combine them into higher level hierarchical structures.","These higher level structures provide a more specific feature and can thus lead to better results in classification or detection.","Although most previous work has focused on hierarchical combinations of low level features, hierarchical structures exist on higher levels as well.","In this work we attempt to automatically discover these higher level structures by finding meaningful object groups using the Minimum Description Length(MDL) principle.","We then use these structures for scene classification and show that we can achieve a higher accuracy rate using them"].join(" ")},{author:"Amir Sadovnik",coauthors:"Andrew Gallagher",pi:"Tsuhan Chen",file:"",year:"2013",link:"https://ieeexplore.ieee.org/document/6619241",title:"It\u2019s Not Polite To Point: Describing People With Uncertain Attributes",citation:"A. Sadovnik, A. Gallagher and T. Chen .\u201cIt\u2019s Not Polite To Point: Describing People With Uncertain Attributes.\u201d, Computer Vision and Pattern Recognition (CVPR), 2013.",abstract:["Visual attributes are powerful features for many different applications in computer vision such as object detection and scene recognition.","Visual attributes present another application that has not been examined as rigorously: verbal communication from a computer to a human.","Since many attributes are nameable, the computer is able to communicate these concepts through language.","However, this is not a trivial task.","Given a set of attributes, selecting a subset to be communicated is task dependent.","Moreover, because attribute classifiers are noisy, it is important to find ways to deal with this uncertainty.","We address the issue of communication by examining the task of composing an automatic description of a person in a group photo that distinguishes him from the others.","We introduce an efficient, principled method for choosing which attributes are included in a short description to maximize the likelihood that a third party will correctly guess to which person the description refers.","We compare our algorithm to computer baselines and human describers, and show the strength of our method in creating effective descriptions."].join(" ")},{author:"Amir Sadovnik",coauthors:"Andrew Gallagher",pi:"Tsuhan Chen",file:"",year:"2013",link:"https://ieeexplore.ieee.org/document/6595886",title:"Not Everybody\u2019s Special: Using Neighbors in Referring Expressions with Uncertain Attributes",citation:"A. Sadovnik, A. Gallagher and T. Chen .\u201cNot Everybody\u2019s Special: Using Neighbors in Referring Expressions with Uncertain Attributes.\u201d, The V&L Net Workshop on Language for Vision, Computer Vision and Pattern Recognition (CVPR), 2013.",abstract:["Referring expression generation is widely considered a basic building block of any natural language generation system.","Generating these phrases, which can point out a single object from a group of objects, has been studied extensively in that community.","However, to build systems which can discuss images in an intelligent way, it is necessary to consider additional factors unique to the visual domain.","In this paper we consider the use of neighbors as anchors to create a referring expression for a person in a group image.","We describe a target person using the people around him, when we cannot find a reliable set of attributes to describe the target himself.","We first present a method for including neighbors in a referring expression, and discuss several ways of presenting this data to a user.","We show through experiments that using descriptions with neighbors can significantly improve the probability of conveying the correct information to a user"].join(" ")},{author:"Amir Sadovnik",coauthors:"",pi:"Tsuhan Chen",file:"",year:"2013",link:"https://ieeexplore.ieee.org/document/6738916",title:"A Visual Dictionary Attack on Picture Passwords",citation:"A. Sadovnik and T. Chen, \u201cA Visual Dictionary Attack on Picture Passwords.\u201d, IEEE International Conference on Image Processing (ICIP) 2013.",abstract:["Microsoft\u2019s Picture Password provides a method to authenticate a user without the need of typing a character based password.","The password consists of a set of gestures drawn on an image.","The position, direction and order of these gestures constitute the password.","Besides being more convenient to use on touch screen devices, this authentication method promises improved memorability in addition to improving the password strength against guessing attacks.","However, how unpredictable is the picture password? In this paper we exploit the fact that different users are drawn to similar image regions, and therefore these passwords are vulnerable to guessing attacks.","More specifically, we show that for portrait pictures users are strongly drawn to use facial features as gesture locations.","We collect a set of Picture Passwords and, using computer vision techniques, derive a list of password guesses in decreasing probability order.","We show that guessing in this order we are able to improve the likelihood of cracking a password within a limited number of guesses."].join(" ")},{author:"Amir Sadovnik",coauthors:"Andrew Gallagher, Devi Parikh",pi:"Tsuhan Chen",file:"",year:"2013",link:"https://dl.acm.org/doi/abs/10.1109/ICCV.2013.268",title:"Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing",citation:"A. Sadovnik, A. Gallagher, D. Parikh and T. Chen. \u201cSpoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing\u201d, International Conference on Computer Vision (ICCV), 2013.",abstract:["In recent years, there has been a great deal of progress in describing objects with attributes.","Attributes have proven useful for object recognition, image search, face verification, image description, and zero- shot learning.","Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars.","However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images.","Specifically, it does not address the important interactions between the binary and relative aspects of an attribute.","In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description.","For each attribute we train a classifier which captures the specific way this attribute should be used.","We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems."].join(" ")},{author:"Kuan-Chuan Peng",coauthors:"Amir Sadovnik, Andrew Gallaher",pi:"Tsuhan Chen",file:"",year:"2015",link:"https://ieeexplore.ieee.org/document/7298687",title:"A Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions",citation:"K. Peng, A. Sadovnik, A. Gallagher, and T. Chen. \u201cA Mixed Bag of Emotions: Model, Predict, and Transfer Emotion Distributions.\u201d Computer Vision and Pattern Recognition (CVPR), 2015.",abstract:["This paper explores two new aspects of photos and human emotions.","First, we show through psychovisual studies that different people have different emotional reactions to the same image, which is a strong and novel departure from previous work that only records and predicts a single dominant emotion for each image.","Our studies also show that the same person may have multiple emotional reactions to one image.","Predicting emotions in \u201cdistributions\u201d instead of a single dominant emotion is important for many applications.","Second, we show not only that we can often change the evoked emotion of an image by adjusting color tone and texture related features but also that we can choose in which \u201cemotional direction\u201d this change occurs by selecting a target image.","In addition, we present a new database, Emotion6, containing distributions of emotions."].join(" ")},{author:"Kuan-Chuan Peng",coauthors:"Amir Sadovnik, Andrew Gallagher",pi:"Tsuhan Chen",file:"",year:"2016",link:"https://ieeexplore.ieee.org/document/7532430",title:"Where Do Emotions Come From? Predicting The Emotion Stimuli Map",citation:"K. Peng, A. Sadovnik, A. Gallagher, and T. Chen. \u201cWhere Do Emotions Come From? Predicting The Emotion Stimuli Map.\u201d IEEE International Conference on Image Processing (ICIP) 2016.",abstract:["Which parts of an image evoke emotions in an observer? To answer this question, we introduce a novel problem in computer vision - predicting an Emotion Stimuli Map (ESM), which describes pixel-wise contribution to evoked emotions.","Building a new image database, EmotionROI, as a benchmark for predicting the ESM, we find that the regions selected by saliency and objectness detection do not correctly predict the image regions which evoke emotion.","Although objects represent important regions for evoking emotion, parts of the background are also important.","Based on this fact, we propose using fully convolutional networks for predicting the ESM.","Both qualitative and quantitative experimental results confirm that our method can predict the regions which evoke emotion better than both saliency and objectness detection."].join(" ")},{author:"Thomas Fuller",coauthors:"",pi:"Amir Sadovnik",file:"",year:"2017",link:"https://ieeexplore.ieee.org/document/8296629",title:"Image level color classification for colorblind assistance",citation:"T. Fuller, A. Sadovnik. \u201cImage level color classification for colorblind assistance.\u201d  IEEE International Conference on Image Processing (ICIP) 2017.",abstract:["The advancement and proliferation of augmented reality lends itself to the development of novel techniques for assistive technologies, especially in the realm of computer vision.","By enhancing a certain part of the view of a person with visual impairment we can assist them in different tasks.","In this work we develop an algorithm to assist people who suffer from color blindness.","We first examine different methods for pixel level color classification to select the one that works the best.","We then improve the color classification rate by optimizing the labeling over the whole image using graph cuts.","Finally, we develop an implementation of the algorithm which can run in real time on Google Glass and show how it can assist those suffering from color blindness."].join(" ")},{author:"Amir Sadovnik",coauthors:"Wassim Gharbi, Thanh Vu",pi:"Andrew Gallagher",file:"",year:"2018",link:"https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w48/Sadovnik_Finding_Your_Lookalike_CVPR_2018_paper.pdf",title:"Finding your Lookalike: Measuring Face Similarity Rather than Face Identity",citation:"A. Sadovnik, W. Gharbi,  T. Vu, A. Gallagher. \u201cFinding your Lookalike: Measuring Face Similarity Rather than Face Identity.\u201d Understanding Subjective Attributes of Data Workshop, Computer Vision and Pattern Recognition (CVPR), 2018.",abstract:["Face images are one of the main areas of focus for computer vision, receiving on a wide variety of tasks.","Although face recognition is probably the most widely researched, many other tasks such as kinship detection, facial expression classification and facial aging have been examined.","In this work we propose the new, subjective task of quantifying perceived face similarity between a pair of faces.","That is, we predict the perceived similarity between facial images, given that they are not of the same person.","Although this task is clearly correlated with face recognition, it is different and therefore justifies a separate investigation.","Humans often remark that two persons look alike, even in cases where the persons are not actually confused with one another.","In addition, because face similarity is different than traditional image similarity, there are challenges in data collection and labeling, and dealing with diverging subjective opinions between human labelers.","We present evidence that finding facial look-alikes and recognizing faces are two distinct tasks.","We propose a new dataset for facial similarity and introduce the Lookalike network, directed towards similar face classification, which outperforms the ad hoc usage of a face recognition network directed at the same task."].join(" ")}];var Q={path:"/publications",component:function(){return i.a.createElement(v.a,{style:{background:"#fff",boxShadow:"0 1px 2px #ccc",width:"80%",margin:"0 0"}},i.a.createElement(W.a,{style:{height:"calc(100vh - 72.7031px)"}},i.a.createElement(q.a.Group,{divided:!0,style:{padding:"1em 2em"}},J.map(function(e,t){return i.a.createElement(Y,{paper:e,key:t})}))))}};var X={path:"/projects",component:function(){return i.a.createElement(v.a,{style:{background:"#fff",boxShadow:"0 1px 2px #ccc",width:"80%",height:"calc(100vh - 72.7031px)",display:"grid"}},i.a.createElement(D.a,{as:"h1",icon:!0,style:{margin:"auto"}},i.a.createElement(y.a,{name:"wrench"}),"Under Construction"))}};var Z={component:function(){return i.a.createElement(v.a,{style:{width:"80%",height:"calc(100% - 72.7031px)",margin:"auto",background:"#fff",justifyContent:"center",display:"flex",alignItems:"center"}},i.a.createElement("h1",null,"Whoops! 404!"))}};function $(){return i.a.createElement(j,null,i.a.createElement(r.a,null,i.a.createElement(o.a,{exact:!0,path:H.path,component:H.component}),[O,z,Q,X].map(function(e,t){return i.a.createElement(o.a,Object.assign({exact:!0,key:"Route-".concat(t)},e))}),i.a.createElement(o.a,{function:Z.component})))}a.d(t,"default",function(){return $})},309:function(e,t,a){"use strict";a.r(t);var n=a(0),i=a.n(n),r=a(22),o=a.n(r),s=a(323);var c=function(e){var t=e.routes;return i.a.createElement(s.a,null,t)},l=(a(170),a(171),a(308).default());o.a.render(i.a.createElement(c,{routes:l}),document.getElementById("root"))}},[[163,1,2]]]);
//# sourceMappingURL=main.19ec1128.chunk.js.map